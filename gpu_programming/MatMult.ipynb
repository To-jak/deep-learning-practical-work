{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copie de MatMult.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nNUkW58qCTs",
        "colab_type": "text"
      },
      "source": [
        "# GPU Programming with CUDA: Matrix Product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsjF8VvmrKVO",
        "colab_type": "text"
      },
      "source": [
        "Execute the cells below to run the code in google colab environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWUsiAohd0P6",
        "colab_type": "code",
        "outputId": "40966291-d6d2-49db-c6b9-4432a07878f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "! pip install git+git://github.com/frehseg/nvcc4jupyter.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/frehseg/nvcc4jupyter.git\n",
            "  Cloning git://github.com/frehseg/nvcc4jupyter.git to /tmp/pip-req-build-fiow7rro\n",
            "  Running command git clone -q git://github.com/frehseg/nvcc4jupyter.git /tmp/pip-req-build-fiow7rro\n",
            "Requirement already satisfied (use --upgrade to upgrade): NVCCPlugin==0.0.1 from git+git://github.com/frehseg/nvcc4jupyter.git in /usr/local/lib/python3.6/dist-packages\n",
            "Building wheels for collected packages: NVCCPlugin\n",
            "  Building wheel for NVCCPlugin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NVCCPlugin: filename=NVCCPlugin-0.0.1-cp36-none-any.whl size=2095 sha256=c4c4c5d937f94d182d11c28e0ae597e69d6e2b955cc2d4f84849129226334380\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uy7hq78v/wheels/a4/a5/24/17a2b61f9a725a10155cc6fca753aae28436921df21fa16114\n",
            "Successfully built NVCCPlugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1EMybILd1VU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o5o1uaXqQQn",
        "colab_type": "text"
      },
      "source": [
        "## Classic product version without shared memory  \n",
        "  \n",
        "In this naive version, we compute the product $C = A * B$ in global memory with the standard matrix multiplication algorithm.\n",
        "\n",
        "* Each thread calculates an element of C\n",
        "* Each thread accesses to a whole line of A and a whole column of B\n",
        "\n",
        "**Downsides of this method**:\n",
        "* The access to data is non-aligned and scattered\n",
        "* Pb of coalescing\n",
        "* Repeated data access"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOmmgILtd1gT",
        "colab_type": "code",
        "outputId": "df3de1a2-9bb1-4af1-8160-2b1e2936758a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#include <time.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "#define SIZE 300\n",
        "/********************** kernel **************************/\n",
        "__global__\n",
        "void matmul(float *A, float *B, float *C, int nb_ColA, int nb_ColB, int nb_RowA, int nb_RowB)\n",
        "{\n",
        "  int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "  int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (row < nb_RowA && col < nb_RowB) {\n",
        "      for(int i = 0; i < nb_ColA; i++){\n",
        "          C[row * nb_ColB + col] += A[row * nb_ColA + i] * B[i * nb_ColB + col];\n",
        "      }\n",
        "  }\n",
        "}\n",
        "\n",
        "/********************** main **************************/\n",
        "int main(void)\n",
        "{\n",
        "  float *A, *B, *C, *gpu_A, *gpu_B, *gpu_C;\n",
        "  int nbRowA, nbRowB, nbColA, nbColB;\n",
        "\n",
        "  \n",
        "  nbRowA = TILE_WIDTH * SIZE;\n",
        "  nbRowB = TILE_WIDTH * SIZE;\n",
        "  nbColA = TILE_WIDTH * SIZE;\n",
        "  nbColB = TILE_WIDTH * SIZE;\n",
        "\n",
        "  A = (float*) malloc(nbRowA * nbColA * sizeof(float));\n",
        "  B = (float*) malloc(nbRowB * nbColB * sizeof(float));\n",
        "  C = (float*) malloc(nbRowA * nbColB * sizeof(float));\n",
        "\n",
        "  /*Allocation de l'espace pour le GPU */\n",
        "  cudaMalloc((void**) &gpu_A, nbRowA * nbColA * sizeof(float));\n",
        "  cudaMalloc((void**) &gpu_B, nbRowB * nbColB * sizeof(float));\n",
        "  cudaMalloc((void**) &gpu_C, nbRowA * nbColB * sizeof(float));\n",
        "\n",
        "  /* Initialisation de A et B*/\n",
        "  for (int i = 0; i < nbRowA * nbColA; i++) {\n",
        "    A[i] = 1.0;\n",
        "  }\n",
        "\n",
        "  for (int i = 0; i < nbRowB * nbColB; i++) {\n",
        "    B[i] = 2.0;\n",
        "  }\n",
        "\n",
        "  /* Copie de A et B sur le GPU */\n",
        "  cudaMemcpy(gpu_A, A, nbRowA * nbColA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(gpu_B, B, nbRowB * nbColB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(gpu_C, C, nbRowA * nbColB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "  /* Lancement du kernel avec mesure du temps */\n",
        " dim3 threadsPerBlock (TILE_WIDTH, TILE_WIDTH);\n",
        " dim3 blocksPerGrid (nbRowA / threadsPerBlock.y, nbRowB / threadsPerBlock.x);\n",
        "\n",
        " /** Timer start**/\n",
        " clock_t start = clock();\n",
        " \n",
        " matmul<<<blocksPerGrid, threadsPerBlock>>>(gpu_A, gpu_B, gpu_C, nbColA, nbColB, nbRowA, nbRowB);\n",
        "\n",
        " clock_t end = clock();\n",
        " double time_taken = ((double) end-start)/CLOCKS_PER_SEC; // in seconds\n",
        " printf(\"\\nkernel took %f seconds to execute \\n\\n\", time_taken);\n",
        " \n",
        "  cudaMemcpy(C, gpu_C, nbRowA * nbColB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  /* Vérification du résultat*/\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < nbRowA * nbColB; i++){\n",
        "      maxError = max(maxError, abs(C[i]- 2*nbRowB));\n",
        "  }\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  /* Libération de la mémoire sur le GPU*/ \n",
        "  cudaFree(gpu_A);\n",
        "  cudaFree(gpu_B);\n",
        "  cudaFree(gpu_C);\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "  free(C);\n",
        " \n",
        " printf(\"%s\\n\", cudaGetErrorString(cudaGetLastError()));\n",
        "}\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "kernel took 0.000033 seconds to execute \n",
            "\n",
            "Max error: 0.000000\n",
            "no error\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8A_fYscqk9w",
        "colab_type": "text"
      },
      "source": [
        "## Shared memory version\n",
        "\n",
        "Optimization: we can divide the matrix in different tiles and compute the product tile after tile.\n",
        "\n",
        "With this, we have:\n",
        "\n",
        "* Cooperative loading of a data tile in shared memory with regular accesses to global memory + quick accesses once in shared memory.\n",
        "* Some synchronization points to ensure that the data of partial results computed by threads are correctly loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPODro7G9CQm",
        "colab_type": "code",
        "outputId": "8c62481b-c908-483b-a634-69a922bb2c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "%%cu\n",
        "#include <stdlib.h>\n",
        "#include <stdio.h>\n",
        "\n",
        "#include <time.h>\n",
        "\n",
        "#include <cuda.h>\n",
        "#include <cuda_runtime.h>\n",
        "#include <helper_cuda.h>\n",
        "\n",
        "#define TILE_WIDTH 32\n",
        "#define SIZE 300\n",
        "/********************** kernel **************************/\n",
        "__global__\n",
        "void matmul(float *A, float *B, float *C, int nb_ColA, int nb_ColB, int nb_RowA, int nb_RowB)\n",
        "{\n",
        "    int blockRow = blockIdx.y;\n",
        "    int blockCol = blockIdx.x;\n",
        "\n",
        "    // Tuile de la matrice C\n",
        "    float* Csub = &C[nb_ColB * TILE_WIDTH * blockRow + TILE_WIDTH * blockCol];\n",
        "\n",
        "    // Valeur de C calculée par un thread\n",
        "    float Cvalue = 0;\n",
        "\n",
        "    // Row et colonne dans une tuile de C\n",
        "    int row = threadIdx.y;\n",
        "    int col = threadIdx.x;\n",
        "\n",
        "    // On parcoure les differentes tuiles de A et B\n",
        "    // qui sont necessaires au calcul de la tuile C\n",
        "    for (int m = 0; m < (nb_ColA / TILE_WIDTH); ++m) {\n",
        "\n",
        "        // Tuile de A\n",
        "        float* Asub = &A[nb_ColA * TILE_WIDTH * blockRow + TILE_WIDTH * m];\n",
        "\n",
        "        // Tuile de B\n",
        "        float* Bsub = &B[nb_ColB * TILE_WIDTH * m + TILE_WIDTH * blockCol];\n",
        "\n",
        "        // Memoire partagee pour stocker les tuiles de A et B\n",
        "        __shared__ float As[TILE_WIDTH][TILE_WIDTH];\n",
        "        __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];\n",
        "\n",
        "        // Chargement de dans la memoire partagee\n",
        "        // des valeurs de A et B par un thread\n",
        "        As[row][col] = Asub[row * nb_ColA + col];\n",
        "        Bs[row][col] = Bsub[row * nb_ColB + col];\n",
        "\n",
        "        // Synchronisation des threads avant calcul pour C\n",
        "        __syncthreads();\n",
        "        // Multiply Asub and Bsub together\n",
        "        for (int e = 0; e < TILE_WIDTH; ++e)\n",
        "            Cvalue += As[row][e] * Bs[e][col];\n",
        "\n",
        "        // Autre Synchronisation avant de charger des nouvelles\n",
        "        // tuiles pour A et B\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    // Ecriture dans C par le thread de la valeur accumulee\n",
        "    Csub[row * nb_ColA + col] = Cvalue;\n",
        "}\n",
        "\n",
        "/********************** main **************************/\n",
        "int main(void)\n",
        "{\n",
        "  float *A, *B, *C, *gpu_A, *gpu_B, *gpu_C;\n",
        "  int nbRowA, nbRowB, nbColA, nbColB;\n",
        "\n",
        "  \n",
        "  nbRowA = TILE_WIDTH * SIZE;\n",
        "  nbRowB = TILE_WIDTH * SIZE;\n",
        "  nbColA = TILE_WIDTH * SIZE;\n",
        "  nbColB = TILE_WIDTH * SIZE;\n",
        "\n",
        "  A = (float*) malloc(nbRowA * nbColA * sizeof(float));\n",
        "  B = (float*) malloc(nbRowB * nbColB * sizeof(float));\n",
        "  C = (float*) malloc(nbRowA * nbColB * sizeof(float));\n",
        "\n",
        "  /*Allocation de l'espace pour le GPU */\n",
        "  cudaMalloc((void**) &gpu_A, nbRowA * nbColA * sizeof(float));\n",
        "  cudaMalloc((void**) &gpu_B, nbRowB * nbColB * sizeof(float));\n",
        "  cudaMalloc((void**) &gpu_C, nbRowA * nbColB * sizeof(float));\n",
        "\n",
        "  /* Initialisation de A et B*/\n",
        "  for (int i = 0; i < nbRowA * nbColA; i++) {\n",
        "    A[i] = 1.0;\n",
        "  }\n",
        "\n",
        "  for (int i = 0; i < nbRowB * nbColB; i++) {\n",
        "    B[i] = 2.0;\n",
        "  }\n",
        "\n",
        "  /* Copie de A et B sur le GPU */\n",
        "  cudaMemcpy(gpu_A, A, nbRowA * nbColA * sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(gpu_B, B, nbRowB * nbColB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(gpu_C, C, nbRowA * nbColB * sizeof(float), cudaMemcpyHostToDevice);\n",
        "\n",
        "\n",
        "  /* Lancement du kernel avec mesure du temps */\n",
        " dim3 threadsPerBlock (TILE_WIDTH, TILE_WIDTH);\n",
        " dim3 blocksPerGrid (nbRowA / threadsPerBlock.y, nbRowB / threadsPerBlock.x);\n",
        "\n",
        " clock_t start = clock();\n",
        "\n",
        " matmul<<<blocksPerGrid, threadsPerBlock>>>(gpu_A, gpu_B, gpu_C, nbColA, nbColB, nbRowA, nbRowB);\n",
        "\n",
        " clock_t end = clock();\n",
        " double time_taken = ((double) end-start)/CLOCKS_PER_SEC; // in seconds\n",
        " printf(\"\\n Kernel took %f seconds to execute\\n\\n\", time_taken);\n",
        "\n",
        "  cudaMemcpy(C, gpu_C, nbRowA * nbColB * sizeof(float), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  /* Vérification du résultat*/\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < nbRowA * nbColB; i++){\n",
        "      maxError = max(maxError, abs(C[i]- 2*nbRowB));\n",
        "  }\n",
        "  printf(\"Max error: %f\\n\", maxError);\n",
        "\n",
        "  /* Libération de la mémoire sur le GPU*/ \n",
        "  cudaFree(gpu_A);\n",
        "  cudaFree(gpu_B);\n",
        "  cudaFree(gpu_C);\n",
        "\n",
        "  free(A);\n",
        "  free(B);\n",
        "  free(C);\n",
        " \n",
        " printf(\"%s\\n\", cudaGetErrorString(cudaGetLastError()));\n",
        "}"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Kernel took 0.000030 seconds to execute\n",
            "\n",
            "Max error: 0.000000\n",
            "no error\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}